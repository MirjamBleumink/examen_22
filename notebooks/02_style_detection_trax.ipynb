{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.settings import StyleSettings\n",
    "from src.data.data_tools import StyleDataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from trax.shapes import signature #Toegevoegd door Mirjam Bleumink, notebook 2 vraag 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = StyleSettings()\n",
    "traindataset = StyleDataset([settings.trainpath])\n",
    "testdataset = StyleDataset([settings.testpath])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 08:52:04.891 | INFO     | src.models.tokenizer:build_vocab:27 - Found 19306 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19308"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "v = tokenizer.build_vocab(corpus, max=20000)\n",
    "len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "# 5 regels code toegevoegd Mirjam Bleumink, notebook 2 vraag 1\n",
    "class StylePreprocessor(tokenizer.Preprocessor): \n",
    "    def cast_label(self, label: str) -> int:\n",
    "        d = {\"humor\": 0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "        label_int = d[label]\n",
    "        return label_int\n",
    "# Einde toevoeging Mirjam Bleumink\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = StylePreprocessor(max=15, vocab=v, clean=tokenizer.clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer)\n",
    "type(x), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "import torch\n",
    "# TODO ~ 2 lines of code\n",
    "# Mirjam Bleumink antwoord op notebook 2 vraag 3\n",
    "# Aanvullende code voor deze vraag (tbv precision) opgenomen in metrics.py\n",
    "\n",
    "metrics=[metrics.Accuracy(),  metrics.F1Score(), metrics.Precision()]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "layer                  input                dtype output               dtype \n",
      "(0) Embedding_19308_128 (32, 15)           ( int32 ) | (32, 15, 128)      (float32)\n",
      "(1) GRU_128             (32, 15, 128)      (float32) | (32, 15, 128)      (float32)\n",
      "(2) BatchNorm           (32, 15, 128)      (float32) | (32, 15, 128)      (float32)\n",
      "(3) AvgLast             (32, 15, 128)      (float32) | (32, 15)           (float32)\n",
      "(4) Dense_4             (32, 15)           (float32) | (32, 4)            (float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:1939: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax_internal._check_user_dtype_supported(dtype, \"zeros\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapeDtype{shape:(32, 4), dtype:float32}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mirjam Bleumink antwoord op notebook 2, vraag 4 - bonus oplossing: model in Trax met gin configuratie\n",
    "# Hiervoor zijn 2 aanvullende bestanden toegevoegd:\n",
    "# - src.models.trax_basemodel \n",
    "# - basemodel.gin\n",
    "\n",
    "import gin\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from src.models import trax_basemodel\n",
    "\n",
    "data_pipeline = trax.data.Serial(trax_basemodel.Cast()) #Tensors veranderen in numpy omdat Trax niet met Tensors werkt\n",
    "trainpipe = data_pipeline(trainstreamer)                #trainstreamer om te gebruiken met Trax\n",
    "testpipe = data_pipeline(teststreamer)                  #teststreamer om te gebruiken met Trax\n",
    "x, y = next(trainpipe)\n",
    "print(type(x),  type(y))\n",
    "\n",
    "trax_model = trax_basemodel.Trax_Basemodel(vocab_size=len(v), d_feature=128, d_out=4)\n",
    "trax_model.init_weights_and_state(signature(x))\n",
    "trax_basemodel.summary(trax_model, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 08:52:05.549 | INFO     | src.data.data_tools:dir_add_timestamp:66 - Logging to ../models/trax/20220705-0852\n",
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/jax/_src/lib/xla_bridge.py:514: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n",
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
      "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 2570178\n",
      "Step      1: Ran 1 train steps in 3.48 secs\n",
      "Step      1: train CategoryCrossEntropy |  1.40070558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/supervised/training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
      "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step      1: eval      CategoryAccuracy |  0.36750000\n",
      "Step      1: eval  CategoryCrossEntropy |  1.40485046\n",
      "\n",
      "Step    100: Ran 99 train steps in 6.18 secs\n",
      "Step    100: train CategoryCrossEntropy |  1.05743742\n",
      "Step    100: eval      CategoryAccuracy |  0.74125000\n",
      "Step    100: eval  CategoryCrossEntropy |  0.73729781\n",
      "\n",
      "Step    200: Ran 100 train steps in 3.93 secs\n",
      "Step    200: train CategoryCrossEntropy |  0.61748552\n",
      "Step    200: eval      CategoryAccuracy |  0.79250000\n",
      "Step    200: eval  CategoryCrossEntropy |  0.52489612\n"
     ]
    }
   ],
   "source": [
    "# Mirjam Bleumink antwoord op notebook 2, vraag 4 - trainen van bonus oplossing: model in Trax\n",
    "\n",
    "from trax.supervised import training\n",
    "from trax.supervised.lr_schedules import warmup_and_rsqrt_decay\n",
    "import gin\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from src.models import trax_basemodel\n",
    "from src.data import data_tools\n",
    "gin.parse_config_file(\"basemodel.gin\")\n",
    "\n",
    "lr = warmup_and_rsqrt_decay(10, 0.01)\n",
    "ll = tl.CategoryCrossEntropy()\n",
    "log_dir_trax = \"../models/trax\"\n",
    "log_dir_trax = data_tools.dir_add_timestamp(log_dir_trax)\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=trainpipe,\n",
    "    loss_layer=tl.CategoryCrossEntropy(),\n",
    "    optimizer=trax.optimizers.Adam(),\n",
    "    lr_schedule=lr\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=testpipe, \n",
    "    metrics=[tl.CategoryAccuracy(), tl.CategoryCrossEntropy()], \n",
    "    n_eval_batches=25\n",
    ")\n",
    "\n",
    "loop = training.Loop(\n",
    "    trax_model,\n",
    "    train_task,\n",
    "    eval_tasks=[eval_task],\n",
    "    output_dir=log_dir_trax\n",
    ")\n",
    "\n",
    "loop.run (200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 23.52222222222222, 'Predicted'),\n",
       " Text(50.722222222222214, 0.5, 'Target')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mirjam Bleumink antwoord op notebook 2 vraag 5 - evalueren van bonus oplossing: model in Trax\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _ in range(30):\n",
    "    x, y = next(testpipe)\n",
    "    yhat = trax_model(x)\n",
    "    yhat = yhat.argmax(axis=-1)\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "y, yhat\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tune the model\n",
    "Don't overdo this.\n",
    "More is not better.\n",
    "\n",
    "Bonus points for things like:\n",
    "- Attention layers\n",
    "- Trax architecture including a functioning training loop\n",
    "\n",
    "Keep it small! It's better to present 2 or 3 sane experiments that are structured and thought trough, than 25 random guesses. You can test more, but select 2 or 3 of the best alternatives you researched, with a rationale why this works better.\n",
    "\n",
    "Keep it concise; explain:\n",
    "- what you changed\n",
    "- why you thought that was a good idea  \n",
    "- what the impact was (visualise or numeric)\n",
    "- explain the impact\n",
    "\n",
    "You dont need to get a perfect score; curiousity driven research that fails is fine.\n",
    "The insight into what is happening is more important than the quantity.\n",
    "\n",
    "Keep logs of your settings;\n",
    "either use gin, or save configs, or both :)\n",
    "Store images in the `figures` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> Tuning van het model</font>\n",
    "### Aanpak en componenten van het tunen\n",
    "Uit de eerdere analyse van de dataset blijkt dat er een groot risico is op overfitting. Hiervoor heb ik enkele maatregelen vastgesteld, waarvan ik enkele in het model wil toepassen:\n",
    "* _Vroeg stoppen_; hiervoor wil ik testen met diverse epoch lengtes zoals beschreven in notebook 2, vraag 4, Ik begin bij 50 en hoog op tot aan 1000, tenzij er eerder al een verlaging in de accuratesse plaats vindt.  \n",
    "* _Reguralisatie door het toevoegen van een dropout in een attention laag_; hiervoor zal ik testen hoe het model presteert zonder attention laag, met attention laag zonder dropout en met attention laag met dropout.\n",
    "* _Gebruik een simpel model_; alhoewel ik in de voorgaande test (dropout in een attention laag) aangeef dat ik een attention laag wil doorvoeren, wil ik ook testen hoe het model presteert zonder attenton laag. Als de accuratesse gelijk is met en zonder attention laag (al dan niet met dropout), dan zal ik kiezen voor het simpele model zonder attention omdat dit beter is voor het voorkomen/minimaliseren van overfitting. \n",
    "* _Toevoegen van synthetische voorbeelden (SMOTE)_; alhoewel dit waarschijnlijk een hele goede methode is om meer waarden voor de categorie proverbs te genereren en daarmee de dataset beter verdeeld te krijgen over de categorieën, heb ik besloten dat dit een activiteit is die ik niet zal doorvoeren in mijn tuning excersitie. Ik heb onderzoek gedaan naar SMOTE en imbalanced data in Trax, maar helaas is daar geen oplossing voor beschikbaar in Trax. Theoretisch zou het wel kunnen met gebruikmaking van SMOTE van imblearn.over_sampling, welke te gebruiken is met numpy arrays. ik heb echter besloten deze stap niet uit te voeren tbv de tijd. \n",
    "\n",
    "Ik wil graag enkele tuning activiteiten uitvoeren die specifiek mogelijk zijn doordat ik Trax gebruik, met name op het gebied van de learning rates. Trax biedt 4 methodes:\n",
    "1. _trax.supervised.lr_schedules.constant_\n",
    "    Deze learning rate vraagt om een _constant_, ik zal testen met de waarden 0.01, 0.001 en 0.0001\n",
    "2. _trax.supervised.lr_schedules.warmup_\n",
    "    Deze learning rate vraagt om een _max_value_, ik zal testen met de waarden 0.01, 0.001 en 0.0001. De waarde voor n_warmup_steps is 10. \n",
    "3. _trax.supervised.lr_schedules.warmup_and_rsqrt_decay_\n",
    "    Deze learning rate vraagt om een _max_value_, ik zal testen met de waarden 0.01, 0.001 en 0.0001. De waarde voor n_warmup_steps is 10. \n",
    "4. _trax.supervised.lr_schedules.multifactor_\n",
    "    Deze learning rate heeft heel veel configuratie mogelijkheden, welke ik niet allemaal zal uitproberen. Ik zal testen met \n",
    "\n",
    "Omdat de learning rate nauw verweven is met de gebruikte optimizer, ben ik voornemens om de learning rates te testen ten opzichte van de diverse optimizers die in trax worden geboden:\n",
    "1. trax.optimizers.adafactor.Adafactor\n",
    "2. trax.optimizers.adam.Adam\n",
    "3. trax.optimizers.base.SGD\n",
    "4. trax.optimizers.momentum.Momentum\n",
    "5. trax.optimizers.rms_prop.RMSProp\n",
    "\n",
    "Bij het tunen maak ik initieel gebruik van het Trax_basemodel, zoals deze eerder in het notebook is opgezet. Deze is als volgt opgebouwd:\n",
    "\n",
    "<img src=\"../figures/trax_basemodel.png\">\n",
    "\n",
    "### Verwachtingen\n",
    "#### Vroeg stoppen; epoch lengte\n",
    "De verwachting is dat er een delicate balans zal zijn tussen lang genoeg trainen om een goed getraind model te hebben, maar niet te lang trainen waardoor het model gaat overfitten. Korter trainen zal mogelijk een negatief gevolg hebben voor de accuratesse van de categorieën 0, 1 en 2, maar van wezenlijk belang zijn voor het voorkomen van overfitten van de categorie 3 en daarmee het generaliserend vermogen van het model. \n",
    "\n",
    "#### Learning rate en optimizers\n",
    "Ik moet eerlijk toegeven dat ik niet echt van tevoren kan bedenken welke learning rate methode en optimizer combinatie het beste zal werken, anders dan dat een variabele methode (methode 2, 3 of 4) waarschijnlijk beter zal presteren dan een constante. Dit experiment is meer ingegeven door interesse/nieuwsgierigheid in de mogelijkheden van Trax en de uitdaging die het mij biedt om dit in Trax uit te voeren. \n",
    "\n",
    "#### Attention laag (met en zonder dropout) & gebruik van een simpel model\n",
    "Normaliter zou ik zeggen dat een attention laag toevoegen beter is voor het model. Ik denk dat dit voor de categorieën 0, 1 en 2 ook wel het geval zal zijn. Echter komt hier ook weer categorie 3 om de hoek kijken. \n",
    "Ik denk dat hier de evaluatie van een model zonder attention, met attention maar zonder dropout en met attention met dropout neerkomt op de gevolgen voor categorie 3. Indien het model even goed presteert met en zonder attention, zal ik kiezen voor een simpeler model zonder attention. \n",
    "\n",
    "### Resultaten\n",
    "#### Vroeg stoppen; epoch lengte\n",
    "De uitkomsten van de testen met de epoch lengte zijn in de volgende tabel inzichtelijk gemaakt. In de tabel staat de algemene locatie van alle logbestanden opgenomen (examen-22/examen-22/models/trax/epoch_lengte), evenals per test de folder waar de logbestanden van die test te vinden zijn. \n",
    "\n",
    "<img src=\"../figures/epoch_length.png\">\n",
    "\n",
    "In de experimenten met de epoch lengte heb ik bij een lengte 550 al geconstateerd dat de accuracy daalde, waardoor doortesten tot aan 1000 niet nodig was. \n",
    "In de eerste set van testen (met ophoging van 50) constateerde ik dat tussen de waarden 350 en 500 de hoogste accuracy werd behaald (*zie bovenste sectie van tabel). Ik heb vervolgens besloten in de reeks van 350 tot 500 met ophoging van 10 per keer te testen, om een optimale waarde voor de epoch lengte te bepalen. Uiteindelijk kwam ik op de beste accuracy uit bij een lengte van 450 (zie onderste sectie van de tabel). In de volgende test (learning rate en optimizer) maak ik daarom gebruik van een epoch lengte van 450. \n",
    "\n",
    "Hieronder volgt de grafische weergave van de train- en evaluatie activiteiten van de test lengte 450 vanuit tensorboard:\n",
    "\n",
    "<img src=\"../figures/Lengte_450.png\">\n",
    "\n",
    "#### Learning rate en optimizers\n",
    "De uitkomsten van de testen met de learning rates en optimizers zijn in de volgende tabel inzichtelijk gemaakt. In de tabel staat de algemene locatie van alle logbestanden opgenomen (examen-22/examen-22/models/trax/learningrate_optimizer), evenals per test de folder waar de logbestanden van die test te vinden zijn. In de tabel is tevens per learning rate methode een heatmap opgenomen om inzichtelijk te maken hoe de performance is met de verschillende configuratie opties ten opzichte van de optimizers. Na de tabel volgt de heatmap van alle learning rate methodes en configuraties ten opzichte van de optimizers. \n",
    "\n",
    "<img src=\"../figures/lr_optimizer.png\">\n",
    "\n",
    "<img src=\"../figures/lr_optimizer_heatmap.png\">\n",
    "\n",
    "Mijn verwachting was dat de multifactor uiteindelijk de beste zou zijn, aangezien die gebruik maakt van de andere 3 methodes en deze combineert tot een nieuwe learning rate, een beetje zoals \"het beste van 3 werelden\". Uit de resultaten blijkt echter dat de warmup and rsqrt decay met max_value 0.01 en Adam als optimizer het beste presteert (accuracy van 0.856 en loss van 0.430). Wat mij wel opvalt is dat ondanks de lagere accuracy van de multifactor methode met Adam als optimizer (0.845), de loss waarde wel de laagste is (0.416). Dat is wel logisch, want wijzigingen in de learning rate zijn juist bedoeld om een lagere loss te behalen.    \n",
    "\n",
    "Hieronder volgt de grafische weergave van de train- en evaluatie activiteiten van de test met Adam optimizer, warmup and rsqrt decay methode en max_value van 0.01 vanuit tensorboard:\n",
    "\n",
    "<img src=\"../figures/Adam_rsqrt-0_01.png\">\n",
    "\n",
    "Omdat de multifactor methode met Adam als optimizer een lagere loss had en hiermee een goede 2e plaats heeft behaald, laat ik hier ook de grafiesche weergave vanuit tensorboard van zien:\n",
    "\n",
    "<img src=\"../figures/Adam_multifactor.png\">\n",
    "\n",
    "De logbestanden van de overige testen zijn allemaal te vinden in de folder welke per test is aangegeven in de resultaten tabel. \n",
    "\n",
    "#### Attention laag (met en zonder dropout) & gebruik van een simpel model \n",
    "Ik had hier heel graag willen melden dat ik een Trax model heb inclusief werkende attention laag, maar helaas heeft Trax mij hierin verslagen (of eigenlijk is mijn Trax kennis hierin tekort geschoten). \n",
    "Ik heb wel een attention laag toegevoegd in Trax_AttentionModel, welke te vinden is in trax_basemodel.py. \n",
    "Dit is de visuele weergave van dit model:\n",
    "\n",
    "<img src=\"../figures/trax_attentionmodel.png\">\n",
    "\n",
    "Helaas krijg ik bij het trainen een foutmelding, die ik ondanks vele pogingen niet kan oplossen (zie ook de output van de cel genaamd \"# Mirjam Bleumink cel gebruikt voor tuning activiteiten met attentionmodel\"):\n",
    "\n",
    "<img src=\"../figures/attention_model_error.png\">\n",
    "\n",
    "Ik heb diverse attention versies geprobeerd (PureAttention, AttentionQKV), inclusief en exclusief paralelle Dense lagen, voor en na de lagen van het basemodel. helaas heeft het allemaal niet geholpen.\n",
    "\n",
    "De keuze is hiermee voor mij gemaakt; <font color=blue> het beste model op basis van de tuning activiteiten die ik heb uitgevoerd is het simpele model (zonder attention), met een epoch lengte van 450, Adam als optimizer en een warmup and rsqrt decay met max_value 0.01 als learning rate. </font>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> Hieronder volgen cellen die zijn gebruikt tijdens de tuning activiteiten. Deze zijn geen onderdeel van mijn antwoorden. </red>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 08:52:26.165 | INFO     | src.data.data_tools:dir_add_timestamp:66 - Logging to ../models/trax/20220705-0852\n",
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/jax/_src/lib/xla_bridge.py:514: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n",
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/supervised/training.py:1388: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison\n",
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/supervised/training.py:1388: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison\n",
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/supervised/training.py:1388: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison\n",
      "/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/supervised/training.py:1388: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison\n"
     ]
    },
    {
     "ename": "LayerError",
     "evalue": "Exception passing through layer Serial (in pure_fn):\n  layer created in file [...]/trax/supervised/training.py, line 1033\n  layer input shapes: (ShapeDtype{shape:(32, 15), dtype:int32}, ShapeDtype{shape:(32,), dtype:int32})\n\n  File [...]/trax/layers/combinators.py, line 90, in forward\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n\nLayerError: Exception passing through layer Serial (in pure_fn):\n  layer created in file [...]/src/models/trax_basemodel.py, line 63\n  layer input shapes: ShapeDtype{shape:(32, 15), dtype:int32}\n\n  File [...]/trax/layers/combinators.py, line 90, in forward\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n\nLayerError: Exception passing through layer BatchNorm (in pure_fn):\n  layer created in file [...]/src/models/trax_basemodel.py, line 66\n  layer input shapes: ShapeDtype{shape:(32, 15, 128), dtype:float32}\n\n  File [...]/trax/layers/normalization.py, line 49, in forward\n    n_batches += 1\n\n  File [...]/site-packages/jax/core.py, line 567, in __add__\n    def __add__(self, other): return self.aval._add(self, other)\n\n  File [...]/_src/numpy/lax_numpy.py, line 4557, in deferring_binary_op\n    return binary_op(self, other)\n\n  File [...]/jax/_src/traceback_util.py, line 162, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n\n  File [...]/jax/_src/api.py, line 473, in cache_miss\n    out_flat = xla.xla_call(\n\n  File [...]/site-packages/jax/core.py, line 1765, in bind\n    return call_bind(self, fun, *args, **params)\n\n  File [...]/site-packages/jax/core.py, line 1776, in call_bind\n    top_trace = find_top_trace(args)\n\n  File [...]/site-packages/jax/core.py, line 999, in find_top_trace\n    top_tracer._assert_live()\n\n  File [...]/jax/interpreters/partial_eval.py, line 1351, in _assert_live\n    raise core.escaped_tracer_error(self, None)\n\njax._src.errors.UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with shape () and dtype int32 to escape.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\n\nThe function being traced when the value leaked was pure_fn at /home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/base.py:542 traced for eval_shape.\n------------------------------\n\nThe leaked intermediate value was created on line /home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/normalization.py:49 (forward). \n------------------------------\n\nWhen the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n------------------------------\n\n/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/fastmath/jax.py:109 (shape_fun)\n/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/base.py:587 (pure_fn)\n\n/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/combinators.py:90 (forward)\n/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/base.py:587 (pure_fn)\n\n/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/normalization.py:49 (forward)\n------------------------------\n\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLayerError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=36'>37</a>\u001b[0m eval_task \u001b[39m=\u001b[39m training\u001b[39m.\u001b[39mEvalTask(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=37'>38</a>\u001b[0m     labeled_data\u001b[39m=\u001b[39mtestpipe, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=38'>39</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[tl\u001b[39m.\u001b[39mCategoryAccuracy(), tl\u001b[39m.\u001b[39mCategoryCrossEntropy()], \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=39'>40</a>\u001b[0m     n_eval_batches\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=40'>41</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=42'>43</a>\u001b[0m loop \u001b[39m=\u001b[39m training\u001b[39m.\u001b[39mLoop(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=43'>44</a>\u001b[0m     trax_model,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=44'>45</a>\u001b[0m     train_task,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=45'>46</a>\u001b[0m     eval_tasks\u001b[39m=\u001b[39m[eval_task],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=46'>47</a>\u001b[0m     output_dir\u001b[39m=\u001b[39mlog_dir_trax\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=47'>48</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=49'>50</a>\u001b[0m loop\u001b[39m.\u001b[39;49mrun (\u001b[39m450\u001b[39;49m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/supervised/training.py:435\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, n_steps)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m task_changed:\n\u001b[1;32m    433\u001b[0m   loss_acc, step_acc \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m--> 435\u001b[0m loss, optimizer_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_step(task_index, task_changed)\n\u001b[1;32m    437\u001b[0m \u001b[39m# optimizer_metrics and loss are replicated on self.n_devices, a few\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m# metrics are replicated (ex: gradients_l2, weights_l2) - i.e. they are\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m# the same across devices, whereas some (ex: loss) aren't because they\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39m# implies the loss here is averaged from this hosts' devices and not\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39m# across all hosts.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m optimizer_metrics, loss \u001b[39m=\u001b[39m fastmath\u001b[39m.\u001b[39mnested_map(\n\u001b[1;32m    447\u001b[0m     functools\u001b[39m.\u001b[39mpartial(tl\u001b[39m.\u001b[39mmean_or_pmean, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_devices),\n\u001b[1;32m    448\u001b[0m     (optimizer_metrics, loss))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/supervised/training.py:632\u001b[0m, in \u001b[0;36mLoop._run_one_step\u001b[0;34m(self, task_index, task_changed)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m task_changed:\n\u001b[1;32m    629\u001b[0m   \u001b[39m# Re-replicate weights and state to synchronize them between tasks.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_weights_and_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mweights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mstate)\n\u001b[0;32m--> 632\u001b[0m (loss, stats) \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mone_step(\n\u001b[1;32m    633\u001b[0m     batch, rng, step\u001b[39m=\u001b[39;49mstep, learning_rate\u001b[39m=\u001b[39;49mlearning_rate\n\u001b[1;32m    634\u001b[0m )\n\u001b[1;32m    636\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callbacks:\n\u001b[1;32m    637\u001b[0m   \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39mcall_at(step):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/optimizers/trainer.py:147\u001b[0m, in \u001b[0;36mTrainer.one_step\u001b[0;34m(self, batch, rng, step, learning_rate)\u001b[0m\n\u001b[1;32m    144\u001b[0m   logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mstate[\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m, state)\n\u001b[1;32m    146\u001b[0m \u001b[39m# NOTE: stats is a replicated dictionary of key to jnp arrays.\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m (new_weights, new_slots), new_state, stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accelerated_update_fn(\n\u001b[1;32m    148\u001b[0m     (weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slots), step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_opt_params, batch, state, rng)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m logging\u001b[39m.\u001b[39mvlog_is_on(\u001b[39m1\u001b[39m) \u001b[39mand\u001b[39;00m ((step \u001b[39m&\u001b[39m step \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    151\u001b[0m   logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mupdated weights[\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m, new_weights)\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/optimizers/trainer.py:217\u001b[0m, in \u001b[0;36m_accelerate_update_fn.<locals>.single_device_update_fn\u001b[0;34m(weights_and_slots, step, opt_params, batch, state, rng)\u001b[0m\n\u001b[1;32m    215\u001b[0m step \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray(step, dtype\u001b[39m=\u001b[39mjnp\u001b[39m.\u001b[39mint32)  \u001b[39m# Needed in TFNP backend.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m weights, slots \u001b[39m=\u001b[39m weights_and_slots\n\u001b[0;32m--> 217\u001b[0m (loss, state), gradients \u001b[39m=\u001b[39m forward_and_backward_fn(\n\u001b[1;32m    218\u001b[0m     batch, weights, state, rng)\n\u001b[1;32m    219\u001b[0m weights, slots, stats \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mtree_update(\n\u001b[1;32m    220\u001b[0m     step, gradients, weights, slots, opt_params, store_slots\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    221\u001b[0m stats[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m loss\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/base.py:605\u001b[0m, in \u001b[0;36mLayer.pure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    603\u001b[0m   \u001b[39m# Skipping 3 lines as it's always the uninteresting internal call.\u001b[39;00m\n\u001b[1;32m    604\u001b[0m   name, trace \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, _short_traceback(skip\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m--> 605\u001b[0m   \u001b[39mraise\u001b[39;00m LayerError(name, \u001b[39m'\u001b[39m\u001b[39mpure_fn\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    606\u001b[0m                    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_caller, signature(x), trace) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mLayerError\u001b[0m: Exception passing through layer Serial (in pure_fn):\n  layer created in file [...]/trax/supervised/training.py, line 1033\n  layer input shapes: (ShapeDtype{shape:(32, 15), dtype:int32}, ShapeDtype{shape:(32,), dtype:int32})\n\n  File [...]/trax/layers/combinators.py, line 90, in forward\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n\nLayerError: Exception passing through layer Serial (in pure_fn):\n  layer created in file [...]/src/models/trax_basemodel.py, line 63\n  layer input shapes: ShapeDtype{shape:(32, 15), dtype:int32}\n\n  File [...]/trax/layers/combinators.py, line 90, in forward\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n\nLayerError: Exception passing through layer BatchNorm (in pure_fn):\n  layer created in file [...]/src/models/trax_basemodel.py, line 66\n  layer input shapes: ShapeDtype{shape:(32, 15, 128), dtype:float32}\n\n  File [...]/trax/layers/normalization.py, line 49, in forward\n    n_batches += 1\n\n  File [...]/site-packages/jax/core.py, line 567, in __add__\n    def __add__(self, other): return self.aval._add(self, other)\n\n  File [...]/_src/numpy/lax_numpy.py, line 4557, in deferring_binary_op\n    return binary_op(self, other)\n\n  File [...]/jax/_src/traceback_util.py, line 162, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n\n  File [...]/jax/_src/api.py, line 473, in cache_miss\n    out_flat = xla.xla_call(\n\n  File [...]/site-packages/jax/core.py, line 1765, in bind\n    return call_bind(self, fun, *args, **params)\n\n  File [...]/site-packages/jax/core.py, line 1776, in call_bind\n    top_trace = find_top_trace(args)\n\n  File [...]/site-packages/jax/core.py, line 999, in find_top_trace\n    top_tracer._assert_live()\n\n  File [...]/jax/interpreters/partial_eval.py, line 1351, in _assert_live\n    raise core.escaped_tracer_error(self, None)\n\njax._src.errors.UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with shape () and dtype int32 to escape.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\n\nThe function being traced when the value leaked was pure_fn at /home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/base.py:542 traced for eval_shape.\n------------------------------\n\nThe leaked intermediate value was created on line /home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/normalization.py:49 (forward). \n------------------------------\n\nWhen the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n------------------------------\n\n/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/fastmath/jax.py:109 (shape_fun)\n/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/base.py:587 (pure_fn)\n\n/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/combinators.py:90 (forward)\n/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/base.py:587 (pure_fn)\n\n/home/mladmin/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/normalization.py:49 (forward)\n------------------------------\n\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError"
     ]
    }
   ],
   "source": [
    "# Mirjam Bleumink cel gebruikt voor tuning activiteiten met basemodel (zonder attention)\n",
    "\n",
    "from trax.supervised import training\n",
    "from trax.supervised.lr_schedules import warmup_and_rsqrt_decay\n",
    "from trax.supervised.lr_schedules import constant\n",
    "from trax.supervised.lr_schedules import warmup\n",
    "from trax.supervised.lr_schedules import multifactor\n",
    "import gin\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from src.models import trax_basemodel\n",
    "from src.data import data_tools\n",
    "gin.parse_config_file(\"basemodel.gin\")\n",
    "\n",
    "#lr = warmup_and_rsqrt_decay(10, 0.0001)\n",
    "#lr = constant(0.0001)\n",
    "lr = warmup(10, 0.1)\n",
    "#lr = multifactor (factors='constant * linear_warmup * rsqrt_decay', constant=0.1, warmup_steps=10, decay_factor=0.5, steps_per_decay=10, steps_per_cycle=100, second_constant=0.01, second_constant_step=10, minimum=0)\n",
    "\n",
    "opt = trax.optimizers.Adam()\n",
    "#opt = trax.optimizers.Adafactor()\n",
    "#opt = trax.optimizers.Momentum() \n",
    "#opt = trax.optimizers.RMSProp()\n",
    "#opt = trax.optimizers.SGD()\n",
    "\n",
    "ll = tl.CategoryCrossEntropy()\n",
    "log_dir_trax = \"../models/trax\"\n",
    "log_dir_trax = data_tools.dir_add_timestamp(log_dir_trax)\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=trainpipe,\n",
    "    loss_layer=tl.CategoryCrossEntropy(),\n",
    "    optimizer=opt,\n",
    "    lr_schedule=lr\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=testpipe, \n",
    "    metrics=[tl.CategoryAccuracy(), tl.CategoryCrossEntropy()], \n",
    "    n_eval_batches=25\n",
    ")\n",
    "\n",
    "loop = training.Loop(\n",
    "    trax_model,\n",
    "    train_task,\n",
    "    eval_tasks=[eval_task],\n",
    "    output_dir=log_dir_trax\n",
    ")\n",
    "\n",
    "loop.run (450)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 14:21:11.715 | INFO     | src.data.data_tools:dir_add_timestamp:67 - Logging to ../models/trax/20220704-1421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial_in4_out2[\n",
      "  Embedding_19308_128\n",
      "  GRU_128\n",
      "  BatchNorm\n",
      "  AvgLast\n",
      "  Dense_4\n",
      "  Parallel_in3_out3[\n",
      "    Dense_128\n",
      "    Dense_128\n",
      "    Dense_128\n",
      "  ]\n",
      "  PureAttention_in4_out2\n",
      "  Dense_128\n",
      "  Serial[\n",
      "    Relu\n",
      "  ]\n",
      "  Dense_4\n",
      "]\n"
     ]
    },
    {
     "ename": "LayerError",
     "evalue": "Exception passing through layer Serial (in init):\n  layer created in file [...]/src/models/trax_basemodel.py, line 74\n  layer input shapes: (ShapeDtype{shape:(32, 15), dtype:int32}, ShapeDtype{shape:(32,), dtype:int64})\n\n  File [...]/trax/layers/combinators.py, line 107, in init_weights_and_state\n    sublayer.init(inputs, use_cache=True))\n\nLayerError: Exception passing through layer Parallel (in init):\n  layer created in file [...]/src/models/trax_basemodel.py, line 80\n  layer input shapes: (ShapeDtype{shape:(32, 4), dtype:float32}, ShapeDtype{shape:(32,), dtype:int64})\n\n  File [...]/trax/layers/combinators.py, line 225, in init_weights_and_state\n    sublayer_signatures = self._allot_to_sublayers(input_signature)\n\n  File [...]/trax/layers/combinators.py, line 270, in _allot_to_sublayers\n    sub_inputs.append(inputs[start])\n\nIndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLayerError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=22'>23</a>\u001b[0m train_task \u001b[39m=\u001b[39m training\u001b[39m.\u001b[39mTrainTask(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=23'>24</a>\u001b[0m     labeled_data\u001b[39m=\u001b[39mtrainpipe,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=24'>25</a>\u001b[0m     loss_layer\u001b[39m=\u001b[39mtl\u001b[39m.\u001b[39mCategoryCrossEntropy(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=25'>26</a>\u001b[0m     optimizer\u001b[39m=\u001b[39mopt,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=26'>27</a>\u001b[0m     lr_schedule\u001b[39m=\u001b[39mlr\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=27'>28</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=29'>30</a>\u001b[0m eval_task \u001b[39m=\u001b[39m training\u001b[39m.\u001b[39mEvalTask(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=30'>31</a>\u001b[0m     labeled_data\u001b[39m=\u001b[39mtestpipe, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=31'>32</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[tl\u001b[39m.\u001b[39mCategoryAccuracy(), tl\u001b[39m.\u001b[39mCategoryCrossEntropy()], \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=32'>33</a>\u001b[0m     n_eval_batches\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=33'>34</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=35'>36</a>\u001b[0m loop \u001b[39m=\u001b[39m training\u001b[39m.\u001b[39;49mLoop(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=36'>37</a>\u001b[0m     trax_attention_model,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=37'>38</a>\u001b[0m     train_task,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=38'>39</a>\u001b[0m     eval_tasks\u001b[39m=\u001b[39;49m[eval_task],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=39'>40</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49mlog_dir_trax\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=40'>41</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c2d6c61622d4d42227d/home/mladmin/code/examen-22/examen-22/notebooks/02_style_detection_trax.ipynb#ch0000014vscode-remote?line=42'>43</a>\u001b[0m loop\u001b[39m.\u001b[39mrun (\u001b[39m450\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/supervised/training.py:248\u001b[0m, in \u001b[0;36mLoop.__init__\u001b[0;34m(self, model, tasks, eval_model, eval_tasks, output_dir, checkpoint_at, checkpoint_low_metric, checkpoint_high_metric, permanent_checkpoint_at, eval_at, which_task, n_devices, random_seed, loss_chunk_size, use_memory_efficient_trainer, adasum, callbacks)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_memory_efficient_trainer:\n\u001b[1;32m    247\u001b[0m   \u001b[39mif\u001b[39;00m _is_uninitialized(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model):\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49minit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_signature)\n\u001b[1;32m    249\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_model\u001b[39m.\u001b[39mrng \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_rng()\n\u001b[1;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m _is_uninitialized(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_model):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-DDG3aTJy-py3.9/lib/python3.9/site-packages/trax/layers/base.py:310\u001b[0m, in \u001b[0;36mLayer.init\u001b[0;34m(self, input_signature, rng, use_cache)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m   \u001b[39m# Skipping 3 lines as it's always the uninteresting internal call.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m   name, trace \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, _short_traceback(skip\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m--> 310\u001b[0m   \u001b[39mraise\u001b[39;00m LayerError(name, \u001b[39m'\u001b[39m\u001b[39minit\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_caller,\n\u001b[1;32m    311\u001b[0m                    input_signature, trace) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mLayerError\u001b[0m: Exception passing through layer Serial (in init):\n  layer created in file [...]/src/models/trax_basemodel.py, line 74\n  layer input shapes: (ShapeDtype{shape:(32, 15), dtype:int32}, ShapeDtype{shape:(32,), dtype:int64})\n\n  File [...]/trax/layers/combinators.py, line 107, in init_weights_and_state\n    sublayer.init(inputs, use_cache=True))\n\nLayerError: Exception passing through layer Parallel (in init):\n  layer created in file [...]/src/models/trax_basemodel.py, line 80\n  layer input shapes: (ShapeDtype{shape:(32, 4), dtype:float32}, ShapeDtype{shape:(32,), dtype:int64})\n\n  File [...]/trax/layers/combinators.py, line 225, in init_weights_and_state\n    sublayer_signatures = self._allot_to_sublayers(input_signature)\n\n  File [...]/trax/layers/combinators.py, line 270, in _allot_to_sublayers\n    sub_inputs.append(inputs[start])\n\nIndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mirjam Bleumink cel gebruikt voor tuning activiteiten met attentionmodel \n",
    "\n",
    "from trax.supervised import training\n",
    "from trax.supervised.lr_schedules import warmup_and_rsqrt_decay\n",
    "from trax.supervised.lr_schedules import constant\n",
    "from trax.supervised.lr_schedules import warmup\n",
    "from trax.supervised.lr_schedules import multifactor\n",
    "import gin\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from src.models import trax_basemodel\n",
    "from src.data import data_tools\n",
    "\n",
    "trax_attention_model = trax_basemodel.Trax_AttentionModel(vocab_size=len(v), d_feature=128, d_out=4, n_heads=1, dropout=0.0, mode='train')\n",
    "print(trax_attention_model)\n",
    "#trax_attention_model.init_weights_and_state(signature(x))\n",
    "#trax_basemodel.summary(trax_model, x)\n",
    "\n",
    "ll = tl.CategoryCrossEntropy()\n",
    "log_dir_trax = \"../models/trax\"\n",
    "log_dir_trax = data_tools.dir_add_timestamp(log_dir_trax)\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=trainpipe,\n",
    "    loss_layer=tl.CategoryCrossEntropy(),\n",
    "    optimizer=opt,\n",
    "    lr_schedule=lr\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=testpipe, \n",
    "    metrics=[tl.CategoryAccuracy(), tl.CategoryCrossEntropy()], \n",
    "    n_eval_batches=25\n",
    ")\n",
    "\n",
    "loop = training.Loop(\n",
    "    trax_attention_model,\n",
    "    train_task,\n",
    "    eval_tasks=[eval_task],\n",
    "    output_dir=log_dir_trax\n",
    ")\n",
    "\n",
    "loop.run (450)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, '0.268'),\n",
       " Text(0, 0, '0.743'),\n",
       " Text(0, 0, '0.676'),\n",
       " Text(0, 0, '0.775'),\n",
       " Text(0, 0, '0.83'),\n",
       " Text(0, 0, '0.844'),\n",
       " Text(0, 0, '0.848'),\n",
       " Text(0, 0, '0.825'),\n",
       " Text(0, 0, '0.874'),\n",
       " Text(0, 0, '0.853'),\n",
       " Text(0, 0, '0.839')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#barplot epoch lengte\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "array = [[0.268,\n",
    "        0.743,\n",
    "        0.676,\n",
    "        0.775,\n",
    "        0.830,\n",
    "        0.844,\n",
    "        0.848,\n",
    "        0.825,\n",
    "        0.874,\n",
    "        0.853,\n",
    "        0.839\n",
    "        ]]\n",
    "df_epochs = pd.DataFrame(array,['accuracy'],\n",
    "                  columns = ['50','100','150','200','250','300','350','400','450','500','550'])\n",
    "\n",
    "epochs = sn.barplot(data = df_epochs,palette = \"Blues\")\n",
    "epochs.set(xlabel = 'epoch length', ylabel = 'accuracy')\n",
    "epochs.bar_label(epochs.containers[0], size = 8)\n",
    "\n",
    "# plt.figure(figsize = (10,7))\n",
    "# sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, '0.268'),\n",
       " Text(0, 0, '0.743'),\n",
       " Text(0, 0, '0.676'),\n",
       " Text(0, 0, '0.775'),\n",
       " Text(0, 0, '0.83'),\n",
       " Text(0, 0, '0.844'),\n",
       " Text(0, 0, '0.848'),\n",
       " Text(0, 0, '0.825'),\n",
       " Text(0, 0, '0.874'),\n",
       " Text(0, 0, '0.853'),\n",
       " Text(0, 0, '0.839')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#barplot epoch lengte - finetunen 350-500\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "array = [[\n",
    "        0.848,\n",
    "        0.828,\n",
    "        0.828,\n",
    "        0.820,\n",
    "        0.830,\n",
    "        0.825,\n",
    "        0.831,\n",
    "        0.851,\n",
    "        0.825,\n",
    "        0.811,\n",
    "        0.859,\n",
    "        0.813,\n",
    "        0.856,\n",
    "        0.815,\n",
    "        0.803,\n",
    "        0.853\n",
    "        ]]\n",
    "df_epochs = pd.DataFrame(array,['accuracy'],\n",
    "                  columns = ['350','360','370','380','390','400','410','420','430','440','450','460','470','480','490','500'])\n",
    "\n",
    "epochs = sn.barplot(data = df_epochs,palette = \"Blues\")\n",
    "epochs.set(xlabel = 'epoch length', ylabel = 'accuracy')\n",
    "epochs.bar_label(epochs.containers[0], size = 8)\n",
    "\n",
    "# plt.figure(figsize = (10,7))\n",
    "# sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, '0.268'),\n",
       " Text(0, 0, '0.743'),\n",
       " Text(0, 0, '0.676'),\n",
       " Text(0, 0, '0.775'),\n",
       " Text(0, 0, '0.83'),\n",
       " Text(0, 0, '0.844'),\n",
       " Text(0, 0, '0.848'),\n",
       " Text(0, 0, '0.825'),\n",
       " Text(0, 0, '0.874'),\n",
       " Text(0, 0, '0.853'),\n",
       " Text(0, 0, '0.839')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#barplot epoch lengte - finetunen 350-500\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "array = [[\n",
    "        0.848,\n",
    "        0.828,\n",
    "        0.828,\n",
    "        0.820,\n",
    "        0.830,\n",
    "        0.825,\n",
    "        0.831,\n",
    "        0.851,\n",
    "        0.825,\n",
    "        0.811,\n",
    "        0.859,\n",
    "        0.813,\n",
    "        0.856,\n",
    "        0.815,\n",
    "        0.803,\n",
    "        0.853\n",
    "        ]]\n",
    "df_epochs = pd.DataFrame(array,['accuracy'],\n",
    "                  columns = ['350','360','370','380','390','400','410','420','430','440','450','460','470','480','490','500'])\n",
    "\n",
    "epochs = sn.barplot(data = df_epochs,palette = \"Blues\")\n",
    "epochs.set(xlabel = 'epoch length', ylabel = 'accuracy')\n",
    "epochs.bar_label(epochs.containers[0], size = 8)\n",
    "\n",
    "# plt.figure(figsize = (10,7))\n",
    "# sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fad48105940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#heatmap - learning rate & optimizer \n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "\n",
    "\n",
    "\n",
    "optimizers = ['Adam','Adafactor','Momentum','RMSProp','SGD']\n",
    "learning_rates = ['constant 0.01','constant 0.001','constant 0.0001','warmup 0.01','warmup 0.001','warmup 0.0001','warmup and rsqrt_decay 0.01','warmup and rsqrt_decay 0.001','warmup and rsqrt_decay 0.0001','multifactor']\n",
    "\n",
    "accuracy = np.array([\n",
    "        [0.855,0.770,0.740,0.788,0.489],\n",
    "        [0.796,0.549,0.608,0.756,0.438],\n",
    "        [0.611,0.421,0.469,0.628,0.380],\n",
    "        [0.825,0.764,0.749,0.838,0.674],\n",
    "        [0.818,0.540,0.515,0.805,0.388],\n",
    "        [0.511,0.389,0.463,0.643,0.441],\n",
    "        [0.856,0.644,0.561,0.788,0.560],\n",
    "        [0.690,0.423,0.513,0.603,0.452],\n",
    "        [0.490,0.235,0.470,0.424,0.405],\n",
    "        [0.845,0.781,0.734,0.820,0.581]     \n",
    "        ])\n",
    "\n",
    "cmap = colors.ListedColormap\n",
    "#YlOrRd\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.imshow(accuracy, cmap = 'YlOrRd')\n",
    "\n",
    "ax.set_xticks(np.arange(len(optimizers)), labels=optimizers)\n",
    "ax.set_yticks(np.arange(len(learning_rates)), labels=learning_rates)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "for i in range(len(learning_rates)):\n",
    "    for j in range(len(optimizers)):\n",
    "        text = ax.text(j, i, accuracy[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", size = 6)         \n",
    "\n",
    "\n",
    "plt.colorbar(heatmap)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fad480ad190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#heatmap - learning rate & optimizer \n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "\n",
    "\n",
    "\n",
    "optimizers = ['Adam','Adafactor','Momentum','RMSProp','SGD']\n",
    "learning_rates = ['multifactor']\n",
    "\n",
    "accuracy = np.array([\n",
    "        #[0.855,0.770,0.740,0.788,0.489],\n",
    "        #[0.796,0.549,0.608,0.756,0.438],\n",
    "        #[0.611,0.421,0.469,0.628,0.380]#,\n",
    "        # [0.825,0.764,0.749,0.838,0.674],\n",
    "        # [0.818,0.540,0.515,0.805,0.388],\n",
    "        # [0.511,0.389,0.463,0.643,0.441]#,\n",
    "        #  [0.856,0.644,0.561,0.788,0.560],\n",
    "        #  [0.690,0.423,0.513,0.603,0.452],\n",
    "        #  [0.490,0.235,0.470,0.424,0.405]#,\n",
    "         [0.845,0.781,0.734,0.820,0.581]     \n",
    "        ])\n",
    "\n",
    "cmap = colors.ListedColormap\n",
    "#YlOrRd\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.imshow(accuracy, cmap = 'YlOrRd')\n",
    "\n",
    "ax.set_xticks(np.arange(len(optimizers)), labels=optimizers)\n",
    "ax.set_yticks(np.arange(len(learning_rates)), labels=learning_rates)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "for i in range(len(learning_rates)):\n",
    "    for j in range(len(optimizers)):\n",
    "        text = ax.text(j, i, accuracy[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", size = 6)         \n",
    "\n",
    "\n",
    "plt.colorbar(heatmap)\n",
    "             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('exam-22-DDG3aTJy-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ef7eee7c1ffccdb050f8336de9a04a9ab88c4d3eb3bee3e0a27c87a184d1d38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
